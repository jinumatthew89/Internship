1) import requests
    from bs4 import BeautifulSoup
    import pandas as pd
    url = "https://en.wikipedia.org/wiki/Main_Page"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    headers = []
    for header in soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6"]):
    text = header.text.strip()
    level = header.name
    headers.append((text, level))
    df = pd.DataFrame(headers, columns=["Text", "Level"])
    print(df)



2) import requests
    from bs4 import BeautifulSoup
    import pandas as pd
    url = "https://presidentofindia.nic.in/former-presidents.htm"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    presidents = []
    for president in soup.select("tbody tr"):
    name = president.select("td")[0].text.strip()
    term = president.select("td")[1].text.strip()
    presidents.append((name, term))
    df = pd.DataFrame(presidents, columns=["Name", "Term"])
    print(df)



3) import requests
   from bs4 import BeautifulSoup
   import pandas as pd
A} url = "https://www.icc-cricket.com/rankings/mens/team-rankings/odi"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    teams = []
    for team in soup.select("tbody tr"):
    name = team.select("td")[1].text.strip()
    matches = team.select("td")[2].text.strip()
    points = team.select("td")[3].text.strip()
    rating = team.select("td")[4].text.strip()
    teams.append((name, matches, points, rating))
    df = pd.DataFrame(teams, columns=["Team", "Matches", "Points", "Rating"])
    df = df.head(10)  # get only top 10 teams
    print(df)
B}  url = "https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    batsman_data = []
    table = soup.find("table", class_="table")
    rows = table.find_all("tr")
    for row in rows[1:11]:
    cells = row.find_all("td")
    batsman = cells[1].text.strip()
    team = cells[2].text.strip()
    rating = cells[3].text.strip()
    batsman_data.append([batsman, team, rating])
    df = pd.DataFrame(batsman_data, columns=["Batsman", "Team", "Rating"])
    print(df)
C}  url = "https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    bowler_data = []
    table = soup.find("table", class_="table")
    rows = table.find_all("tr")
    for row in rows[1:11]:
    cells = row.find_all("td")
    bowler = cells[1].text.strip()
    team = cells[2].text.strip()
    rating = cells[3].text.strip()
    bowler_data.append([bowler, team, rating])
    df = pd.DataFrame(bowler_data, columns=["Bowler", "Team", "Rating"])
    print(df)



4) import requests
    from bs4 import BeautifulSoup
    import pandas as pd
A}  url = "https://www.icc-cricket.com/rankings/womens/team-rankings/odi"
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    teams = []
    for team in soup.select("tbody tr"):
    name = team.select("td")[1].text.strip()
    matches = team.select("td")[2].text.strip()
    points = team.select("td")[3].text.strip()
    rating = team.select("td")[4].text.strip()
    teams.append((name, matches, points, rating))
    df = pd.DataFrame(teams, columns=["Team", "Matches", "Points", "Rating"])
    df = df.head(10)  # get only top 10 teams
    print(df)
     
B}  url = "https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    batsman_data = []
    table = soup.find("table", class_="table")
    rows = table.find_all("tr")
    for row in rows[1:11]:
    cells = row.find_all("td")
    batsman = cells[1].text.strip()
    team = cells[2].text.strip()
    rating = cells[3].text.strip()
    batsman_data.append([batsman, team, rating])
    df = pd.DataFrame(batsman_data, columns=["Batsman", "Team", "Rating"])
    print(df)
 
C}  url = "https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    bowler_data = []
    table = soup.find("table", class_="table")
    rows = table.find_all("tr")
    for row in rows[1:11]:
    cells = row.find_all("td")
    bowler = cells[1].text.strip()
    team = cells[2].text.strip()
    rating = cells[3].text.strip()
    bowler_data.append([bowler, team, rating])
    df = pd.DataFrame(bowler_data, columns=["Bowler", "Team", "Rating"])
    print(df)
    
    
    
 5) import requests
    from bs4 import BeautifulSoup
    import pandas as pd
    url = "https://www.cnbc.com/world/?region=world"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    articles = soup.find_all("div", class_="Card-titleContainer")
    headlines = []
    times = []
    links = []
    headline = article.find("a").text.strip()
    headlines.append(headline)
    time = article.find("time").text.strip()
    times.append(time)
    link = article.find("a")["href"]
    links.append(link)
    data = {"Headline": headlines,"Time": times,"News Link": links}
    df = pd.DataFrame(data)
    print(df)   
    
    
    
6)  import requests
    from bs4 import BeautifulSoup
    import pandas as p
    url = "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, "html.parser")
    articles_container = soup.find("div", class_="pod-listing")
    titles = []
    authors = []
    dates = []
    urls = []
    for article in articles_container.find_all("li"):
    title = article.find("h3").text.strip()
    titles.append(title)
    author = article.find("span", class_="text-xs").text.strip()
    authors.append(author)
    date = article.find("span", class_="text-xs").find_next_sibling("span").text.strip()
    dates.append(date)
    url = article.find("a")["href"]
    urls.append(url)
    data = {"Paper Title": titles,"Authors": authors,"Published Date": dates,"Paper URL": urls}
    df = pd.DataFrame(data)
    print(df)
    


7)  import requests
    from bs4 import BeautifulSoup
    import pandas as pd
    url = "https://www.dineout.co.in"
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')
    cuisines = soup.find_all('span', class_='double-line-ellipsis')
    locations = soup.find_all('span', class_='double-line-ellipsis')
    ratings = soup.find_all('span', class_='rating-value')
    image_urls = soup.find_all('img', class_='img-responsive')
    restaurant_list = []
    cuisine_list = []
    location_list = []
    rating_list = []
    image_url_list = []
    for name in restaurant_names: restaurant_list.append(name.text.strip())
    for cuisine in cuisines: cuisine_list.append(cuisine.text.strip())
    for location in locations: location_list.append(location.text.strip())
    for rating in ratings: rating_list.append(rating.text.strip())
    for image in image_urls: image_url_list.append(image['src'])
    data = {'Restaurant Name': restaurant_list,'Cuisine': cuisine_list,'Location': location_list,'Ratings': rating_list,
    'Image URL': image_url_list}
    df = pd.DataFrame(data)
    print(df) 